---
title: "Crimes Time Series"
author: "Kristin Fesmire"
date: "2023-08-25"
output: html_document
---

```{r, warning = FALSE, message = FALSE}
rm(list = ls())

library(stringr)
# library(EnvStats)
# library(ggpubr)
library(ggplot2)
library(forecast)
library(reshape2)

# df <- read.csv('C:/Users/krtfe/Downloads/Crimes_-_2023-Updated.csv')
df <- read.csv("C:/Users/krtfe/Downloads/Crimes_-_2023 (ret. 082023).csv")

cat('Pre-cleaning summary:\n\n')
df %>% summary %>% print
# remove duplicate rows, removed 0 rows
df <- dplyr::distinct(df)

# remove duplicate rows by case number, removed 12 rows, from 150712 to 150700
df <- df[!duplicated(df$Case.Number),]

# simplify data, remove columns that aren't useful for current project
df <- df[c(3, 6, 7:10, 12:14)]

# removed columns  so the data could be imported to github
# write.csv(df, 'C:/Users/krtfe/Downloads/Crimes_-_2023-8-20.csv')

# remove rows with NA values, removed 3 rows, 150700 rows -> 150679 rows
df <- na.omit(df)

# adding useful columns, dates, times, time of day
dates <- str_split(df$Date, pattern = ' ', simplify = TRUE)[,1]
times <- str_split(df$Date, pattern = ' ', simplify = TRUE)[,2]
time_of_day <- str_split(df$Date, pattern = ' ', simplify = TRUE)[,3]

# add the useful columns and transform data types
df['Date'] <- as.Date(dates, format = '%m/%d/%Y')
df['Time'] <- times
df['Time of Day'] <- time_of_day

# set dataframe such that it only includes months from january to july
# from 150679 rows -> 147596
df <- df[df$Date < lubridate::ymd("2023-08-01"),]

cat('\n\nPost-cleaning summary:\n\n')
df %>% summary %>% print

# data frame representation
df %>% head

```
\
\


### Separate data frame for (specific variable) counts by dates:

```{r}
# second data frame, number of crimes

# start with the unique dates and their counts
numCrimes <- table(df$Date)
dfCounts <- data.frame(numCrimes)
colnames(dfCounts) <- c('Date', 'Number of Crimes')

# make row names the dates, for convenience
row.names(dfCounts) <- dfCounts$Date

# renaming column names for consistency
colnames(dfCounts) <- str_to_title(colnames(dfCounts))

# printing the counts dataset
dfCounts %>% head

```


Use old data for analyzing this girl, make it easier make it sexier babes
### Separate data frame for (specific variable) counts by dates, from 2001 to 2022:
```{r, warning = FALSE, message = FALSE}

dfTotal <- read.csv("C:/Users/krtfe/Downloads/Crimes_-_2001_to_Present.csv")

# adding useful columns, dates, times, time of day
# dfTotal['Date'] <- str_split(dfTotal$Date, pattern = ' ', simplify = TRUE)[,1]
dfTotal['Date'] <- substr(dfTotal$Date, 1, 10)

# transform data types
dfTotal['Date'] <- as.Date(dfTotal$Date, format = '%m/%d/%Y')

# set dataframe such that it only includes months from january to july
# from 150679 rows -> 147596
dfTotal <- dfTotal[dfTotal$Date < lubridate::ymd("2023-01-01"),]

dfTotal <- sort(dfTotal)

# data frame representation
dfTotal %>% head
dfTotal %>% tail
    

```



```{r}
# start with the unique dates and their counts
numCrimesTotal <- table(dfTotal)
dfCountsTotal <- data.frame(numCrimesTotal)
colnames(dfCountsTotal) <- c('Date', 'Number of Crimes')

dfCountsTotal %>% head

```

### Add a variable for when there is a leap day.
```{r, warning = FALSE, message = FALSE}

leapDays <- grepl('02-29', dfCountsTotal$Date)
dfCountsTotal['Leap Days'] <- ifelse(leapDays, 1, 0)
# t-test to determine significance from each year's distribution
# 

```



### Visualization of 2023 Crimes Time Series:
```{r}


dfCountsTS <- ts(dfCounts[1:2])
lg <- lm(dfCountsTS[,2] ~ dfCountsTS[,1])

plot(dfCountsTS[,1], dfCountsTS[,2], 
     xlab = 'Dates',
     ylab = 'Number of Crimes', 
     type = 'l')

# for trend analysis
summary(lg)
anova(lg)


```


# Visualization of 2001 - 2022 Crimes Time Series: 
```{r}
# start with the unique dates and their counts
dfTotalTS <- ts(dfCountsTotal, 
                start = c(2001, 1),
                end = c(2022, 365),
                frequency = 365,
                deltat = 1/365)

autoplot(dfTotalTS[,2], 
         ylab = 'Number of Crimes',
         main = 'Time Series for Total Crimes by Day')


```

The initial time series visually shows a downward trend and seasonality. 
The seasonality seems to be yearly. 

### Is the distribution normally distributed? 
```{r}

hist(dfCountsTotal$'Number of Crimes', breaks = 100)
hist(dfCounts$`Number Of Crimes`, breaks = 100)
hist(diff(dfCounts$`Number Of Crimes`), breaks = 100)
qqnorm(dfCounts$`Number Of Crimes`)
qqline(dfCounts$`Number Of Crimes`)
qqnorm(dfCountsTotal$`Number of Crimes`)
qqline(dfCountsTotal$`Number of Crimes`)
qqnorm(diff(dfCountsTotal$`Number of Crimes`))
qqline(diff(dfCountsTotal$`Number of Crimes`))


```

Doesn't seem to be normally distributed due to a decreasing trend. 
It is normally distributed year by year, though.


### ACF, PACF, ACVF
```{r}

ggAcf(dfTotalTS[,2], lag.max = 100)


```

### Differenced by 1
```{r}

a <- diff(dfTotalTS[,2], lag = 1, differences = 1)
autoplot(a)
ggAcf(a, lag.max = 100)
ggAcf(a, lag.max = 100, type = 'partial')


```

The ACF plot seems to show non-stationarity.
There is a slight downward slope in the ACF plot as there is in the time
series plot. There also seems to be slight divots in the plot, which may 
be explained by the potential seasonality of the time series. 

### Augmented Dickey Fuller Test for Stationarity
```{r}

# ADF test for stationarity, hospitalizations
cat('ADF Test for Cases, Segment 1:\n')
tseries::adf.test(dfTotalTS[,2])

cat('ADF Test for Cases, Segment 1:\n')
tseries::adf.test(a)


```

The ADF test seems to conclude stationarity

### Seasonality
```{r}

ggseasonplot(dfTotalTS[,2]) +
  ylab('Crime Counts') +
  xlab('Month') + 
  ggtitle("Seasonality Plot: Chicago Crimes")


```
The seasonality plot, on the other hand, seems to show a clear curve
throughout the course of the year. 

### Moving Average calculations for trend
Moving averages will be calculated year by year due to seasonality within each year.
```{r}

# calculate moving averages by year

# first create a table for year by year crime totals 
ybyTotal <- substr(dfTotal, 1, 4)
ybyTotFreq <- table(ybyTotal)
ybyFreqDF <- data.frame(ybyTotFreq)
colnames(ybyFreqDF) <- c('Date', 'Number of Crimes')

ybyFreqDF %>% head
    

```

# Calculation of Moving Averages of order m = 3 and Seasonal Variations
```{r}

ybyFreqDF['Moving Average'] <- NA

for (i in 2:(nrow(ybyFreqDF)-1)) {
    y1 <- ybyFreqDF[i-1, 'Number of Crimes']
    y2 <- ybyFreqDF[i, 'Number of Crimes']
    y3 <- ybyFreqDF[i+1, 'Number of Crimes']
    ybyFreqDF[i, 'Moving Average'] <- (y1 + y2 + y3)/3
}

ybyFreqDF['Seasonal Variation'] <- NA

for (i in 2:(nrow(ybyFreqDF)-1)) {
    y1 <- ybyFreqDF[i, 'Number of Crimes']
    y2 <- ybyFreqDF[i, 'Moving Average']

    ybyFreqDF[i, 'Seasonal Variation'] <- y1-y2
}


ybyFreqDF

```

# Moving Average Trend
```{r}
MATrend <- c()

for (i in 2:(nrow(ybyFreqDF)-2)) {
    y1 <- ybyFreqDF[i, 'Moving Average']
    y2 <- ybyFreqDF[i+1, 'Moving Average']
    MATrend <- c(MATrend, y1-y2)
}

lg <- lm(MATrend ~ c(1:length(MATrend)))
summary(lg)
plot(1:length(MATrend), MATrend)
abline(mean(MATrend), 0)

plot(1:22, ybyFreqDF$`Seasonal Variation`)


```
This trend is pretty varying year by year, and there doesn't appear to be an absolute trend value for the model. 

The above creation of moving averages helped to better understand the model, but didn't 
By this, the model was decomposed

### Decomposition
```{r}

# decomposed time series
decompTS <- decompose(dfTotalTS[,2])
autoplot(decompTS)


```


### Decomp
```{r}

seasTS <- na.omit(decompTS$random)
autoplot(seasTS)
ggAcf(seasTS, lag.max = 100)

```
Removing the trend and stationarity from the time series seems to leave some
potential non-constant variance, as the variance is larger at the beginning 
of the time series and smaller at the end. 

The seasonality wasn't eliminated in the last season due to the nature of moving averages






### Fitting the model to find the ideal Arima model
```{r}

crimes01_19 <- ts(dfCountsTotal, 
                  start = c(2001, 1),
                  end = c(2019, 365),
                  frequency = 365,
                  deltat = 1/365)

arimaTest <- auto.arima(crimes01_19[,2], 
                        D = 1,
                        d = 1)
summary(arimaTest)


```

this says that the best model is (3, 1, 1)(0, 1, 0). I will make an Arima
model based on this


keep this for pretty reasons
```{r}

dfTotalTSg <- ts(dfCountsTotal[6940:nrow(dfCountsTotal),],
                start = c(2020, 12),
                end = c(2022, 365),
                frequency = 365,
                deltat = 1/365)
# 

autoplot(dfTotalTSg[,2], col = 'blue') + 
    autolayer(fore1$mean)

```

# Creating the final model
```{r}

crimesArima <- Arima(crimes01_19[,2], 
                     order = c(3, 1, 1), 
                     seasonal = c(0, 1, 0),
                     xreg = crimes01_19[,3])
summary(crimesArima)

foreCrimes <- forecast(crimesArima, 
                       h = 365,
                       xreg = crimes01_19[,3])

crimes01_22 <- ts(dfCountsTotal,
                  start = c(2001, 1),
                  end = c(2022, 365),
                  frequency = 365,
                  deltat = 1/365)

autoplot(crimes01_22[,2]) + 
    autolayer(foreCrimes$mean)

```



### Check residuals for normality
```{r}

# ARIMA for entire cases time series
checkresiduals(crimesArima)

```


### LSTM idk if i really wanna build another model :/
```{r}

library(keras)
library(tensorflow)



```


### TBATS
```{r}

crimesTbats <- tbats(dfTotalTS)
fore2 <- forecast(crimesTbats, h=365*2)
plot(fore2, ylab="thousands of barrels per day")

```


### ETS
```{r}



```

### STLM
```{r}



```
